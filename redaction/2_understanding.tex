\chapter{Understanding source code}

Aesthetics in source code are thus primarily used to alleviate cognitive load. In the previous chapter, we've seen that there is a focus on understanding when it comes to aesthetic standards: whether obfuscating or illuminating, understanding is a key determinant in the value judgment. In this chapter, we focus on the reason for such a cognitive load in the first place, before surveying the means—both mechanistic and linguistic—that programmers deploy in order to relieve such a load.

This is related to one of the essential features of software: it must be \emph{functional}. As mentioned in our discussion of the differences between source code and software in the introduction, source code is the latent description of what the software will ultimately \emph{do}. Similarly to sheet music, or to cooking recipes\footnote{Recipes are a recurring example taken to communicate the concept of an algorithm to non-experts \citep{zeller_algorithms_2020}}, they require to be put into action in order for their users (musicians and cooks, respectively) to assess their value. Buggy, or dysfunctional software, are going to be of less value than correct software \citep{hill_what_2016}, regardless of how aesthetically pleasing the source is.

The assessment of whether a software functions correctly is essentially an assessment of whether what the software does is what the software is supposed to do, which in turn entails knowing what it does, what it is supposed to do, and being able to tell whether these two things are aligned. Any value judgment regarding the aesthetics of the source code at hand would be subject to whether or not the software functions correctly, and such judgment can be rendered moot if that is not the case.

Once one has decided on a benchmark to assess the functionality of the code against, which itself exists as an abstract behavior, one must then determine the actual behavior of the source code at hand once it is executed. This chapter thus examines what goes into understanding source code. The first part will lay out our definition of understanding, presneting it as a dual phenomenon, between formalism and contextualism. Starting with 20\^{th} century epistemology, we will see that an exclusively rational, cognitivist perspective on the nature of understanding, as it has been hailed by theoretical computer science research, shows its limitation when confronted with practice. Having highlighted this tension, we then turn to how a single individual might understand the phenomenon of computation, both on an ontological level, and on a psychological level. The ontological approach will show some of the features of software which make it inherently hard to comprehend, while the psychological approach will show how such a comprehension takes place in a varity of programmers. Finally, we will conclude with the means that programmers deploy to grasp the concepts at play with software: starting from metaphors used by the general public, we will go down this ladder of abstraction in order to reach the technical apparatuses used in the development and inspection of source code.

The main questions that this chapter addresses are thus as follows: given a certain nature of knowledge acquisition, what are some of the features of computers that make them hard to graps, and what kind of techniques are deployed in order to address these hurdles and in order to understand what the code is actually doing. This will have us investigate the relationship of knowing and doing, the nature of computation (what do computers and source code do?) and its relationship to the world as it appears to us (how does modelling and abstraction translate a problem domain into software?).

% first the definition of understanding
\section{Understanding as embodied action} %22k char

This section focuses on our definition of understanding, which relies on two main aspects: a formal, abstract understanding, and a more subjective tacit one. We will see how the former had some traction in computer sciences circles, while the second gained traction in programming circles. To support those two approaches, we first trace back the genealogy of understanding in theoretical computer science, before outlining how concrete, situatedness hints at an alternative tradition.

\subsection{Between formal and informal} %10k

\subsubsection{Theoretical foundations of formal understanding}

To start this inquiry, we go back to the early 20\^{th} century in Cambridge, when the theoretical roots of modern computation were being laid by philosophers of logic and mathematicians, such as Bertand Russell, Ludwig Wittgenstein, and Alan Turing.

Wittgenstein, in particular, bases his argumentation in his \emph{Tractatus Logico-philosophicus} on the fact that much of the problems in philosophy are rather problems of understanding between philosophers—if one were to express oneself clearly, and to articulate one's through logically, a common conclusion could be reached without much effort:

\begin{quote}
    Most questions and propositions of the philosophers result from the fact that we do not understand the logic of our language. \cite{wittgenstein_tractatus_2010}
\end{quote}

Language and logic, as exposed here, are closely connected, Articulated in separate points and sub-points, his work conjugates aphorisms with logical propositions depending on one another, developing from broader statements into more specific precisions, Wittgenstein hints at the intertwining of language as a form of logic, and as logic as a form of language. In this, he follows in the footsteps of Gottfried Leibniz's \emph{Ars Combinatoria}, in which he views reasoning and inter-subjective understanding as a formal problem. Quoted by Russell, Leibniz notes that:

\begin{quote}
    If we had it [a characteristica universalis], we should be able to reason in metaphysics and morals in much the same way as in geometry and analysis... If controversies were to arise, there would be no more need of disputation between two philosophers than between two accountants [...] Let us calculate.\cite{russell_logical_1950}
\end{quote}

Wittgenstein presents a coherent, articulated theory of meaning through the use of mathematical philosophy, and logic, and his work fits with that of Russell\footnote{In his \emph{Principia Mathematica}, he lays out a theory of logical expression} and Frege\footnote{The \emph{Begriffschrift} similarly attempts to constitute a language in which all scientific statements could be evaluated \cite{korte_frege_2010}, while \emph{Über Sinn und Bedeutung} clarifies the semantic uncertainties between a specific sentence and how it means, or refers to a concept}; even though these are different theories, they articulate as an endeavour based on formal propositions in order to establish truth-values. Such articulation were a direct influence in the work on mathematician Alan Turing—who studied at Cambridge and followed some of Wittgenstein's lectures—, as he developed his own formal system for solving complex, abstract problems, manifested as a symbolic machine\cite{turing_computable_1936}.

Even though the design of the Turing machine is a subsequent step engagement with the question of understanding in the philosophical sense, but rather in the practical sense. Indeed, it is a response to the questions of translation (of a problem) and of implementation (of a solution). This formal approach to instructing machines to operate on logic then prompted Turing to investigate the question of intelligence and comprehension in \emph{Computing Machinery and Intelligence}\cite{turing_computing_2009}. In it, he translates the hazy term of "thinking" machines into that of "conversing" machines, conversation being a practical human activity which involves listening, understanding and answering (i.e. input, process and output). This conversational test, a benchmark for machine intelligence in the years to come, does rely on the need for a machine to \emph{understand} what is being said. Throughout the article, however, Turing does not address the need for a purely formal approach of whether or not a problem can be translated, as Leibniz would have it, into atomistic symbols which would be provided as an input to a digital computer. Such a process of translation would rely on a formal approach, similar to that laid out in the \emph{Tractatus Logico-philosophicus}, or on Frege's formal language described in the \emph{Begriffschrift}. Following a cartesian approach, the idea in both authors is to break down a concept, or a proposition, into sub-propositions, in order to recursively\footnote{although it was not called as such at the time} establish the truth of each of these sub-propositions, and then re-assembled to deduce the truth-value of the original proposition.

With these sophisticated syntactic systems developed a certain approach to cognition—Turing clearly establishing parallels between the digital computer and the human brain—, and they were used in order to clearly understand and operate on mathematical and logical statements. Beyond their conceptual unity, one should also note their stylistic unity.

% To understand a proposition means to know what is the case, if it is true. (One can therefore understand it without knowing whether it is true or not.) One understands it if one understands its constituent parts. p.41#4.024
Logical calculus, as the integration of the symbol into relationships of many symbols formally takes place through a stylistic mechanism, the \emph{list}. Each of the works quoted above are structured in terms of lists and sub-lists, representing the stylistic pendant to the epistemological approach of related, atomistic sub-propositions. A list, far from being an innate way of organizing information in humans, is a particular approach to language: extracting elements from their original, situated existence, and reconnecting ways in very rigorous, strictly-defined ways. As Jack Goody writes in \emph{The Domestication of the Savage Mind},

\begin{quote}
    [List-making] [...] is an example of the kind of decontextualization that writing promotes, and one that gives the mind a special kind of lever on 'reality'.\cite{goody_domestication_1977}
\end{quote}

As inventories, early textbooks, administrative documents as public mnemotechnique, the list is a way of taking symbols, pictorial language elements in order to re-assemble them to reconstitute the world, then re-assemble it from blocks, following an assumption that the world can always be decomposed into smaller, discreete and \emph{conceptually coherent} units. The list, Goody continues, establish clear-cut boundaries, they are simple, they are abstract and discontinuous. Being based on some singular, symbolical entity, applying logical calculus to lists and their symbols, and doing so in a computing environment, becomes the next step. Indeed, the engineering development of digital computers in post-war United States as described in \ref{subsec:software-developers}, allowed for the putting into practice of these languages, in the budding field of artificial intelligence (AI).

% then a transition to AI and abstract theories of mind and cognitivism (5000)
\subsubsection{Practical attempts at implementing formal understanding}

This putting into practice took the form of subsequent programming languages, relying on a certain conception of human cognition—abstract, logical, as shown above.

IPL, the Information Processing Language, was created by Allen Newell, Cliff Shaw and Herbert A. Simon. IPL's fundamental construct is also the symbol, which at the time were more or less mapped to physical addresses and cells in the computer's memory, and not yet decoupled from hardware.

% add example of IPL
A link between the ideas exposed in the writing of the mathematical logicians and the actual design and construction of electrical machines activating these ideas, IPL was originally designed to demonstrate the theorems of Russell's \emph{Principia Mathematica}, along with a couple of early AI programs, such as the \emph{Logic Theorist}, the \emph{General Problem Solver}. More a proof of concept than a versatile language, IPL was then quickly replaced by LISP as the linguistic means to express intelligence in digital computers.

% add figure on what lisp looks like
LISP (\emph{LIst Processor}) was developed in 1956 was Joseph McCarthy\footnote{McCarthy coined the phrase \emph{Artificial Intelligence} during the 1956 Dartmouth workshop}. The base structural elements of LISP are not symbols, but lists (of symbols, of lists, of nothing), and they themselves act as symbols (e.g. the empty list). [http://jmc.stanford.edu/articles/lisp/lisp.pdf] and [http://web.cse.ohio-state.edu/~rountev.1/6341/pdf/Manual.pdf] By manipulating those lists recursively—that it, processing something in terms of itself—Lisp highlights even further this tendency to separate itself from the problem domain. This is facilitated by its atomistic and relational structure: in order to solve what it has do, it evaluates each symbol and traverses a tree-structure in order to find a terminal symbol.

% add figure on tree structure of language
This sort of heuristic is quite similar to the approach suggested by Noam Chomsky in his \emph{Syntactic Structures}, where he posits the tree structure of language, as a decomposition of sentences until the smallest conceptually coherent parts (e.g. Phrase -> Noun-Phrase + Verb-Phrase -> Article + Substantive + Verb-Phrase). The style is similar, insofar as it proposes a general ruleset (or the at least the existence of one) in order to construct complex structures through simple parts.

Through its direct manipulation of conceptual units upon which logic operations can be executed, LISP became the language of AI, an intelligence conceived first and foremost as logical understanding. The use of LISP as a research tool culminated in the \emph{SHRDLU} program, a natural language understanding program built in 1968-1970 by Terry Winograd which aimed at tackling the issue of situatedness—AI can understand things abtractly through logical mathematics, but can it apply these rules within a given context? The program had the particularity of functioning with a "blocks world" a highly simplified version of a physical environment—bringing the primary qualities of abstraction into solid grasp. The computer system was expected to take into account the rest of the world and interact in natural language with a human, about this world (\emph{Where is the red cube?} \emph{Pick up the blue ball}, etc.). While incredibly impressive at the time, \emph{SHDRLU}'s success was nonetheless relative. It could only succeed at giving barely acceptable results within highly symbolic environments, devoid of any noise. In 2004, Terry Winograd writes:

\begin{quote}
    There are fundamental gulfs between the way that SHRDLU and its kin operate, and whatever it is that goes on in our brains. I don’t think that current research has made much progress in crossing that gulf, and the relevant science may take decades or more to get to the point where the initial ambitions become realistic. \cite{nilsson_quest_2009}
\end{quote}

This attempt, since the beginning of the century, to enable thinking, clarify understanding and implement it in machines, had first hit an obstacle. The world—the problem domain—exhibits a certain complexity which did not seem to be easily translated into singular, atomistic symbols. Around the same time, however, was developed another approach to the intricacies of cognition.

Warren McCullough's seminal paper, \emph{A logical calculus of the ideas immanent in nervous activity}, co-written with Walter Pitts, is such an example. He presents a connection with the systematic, input-output procedures dear to cybernetics with the predicate logic writing style of Russell and others\citep{mcculloch_logical_1990}. This attachment to input and output, to their existence in complex, inter-related ways, rather than self-contained propositions is, interestingly, rooted in his activy as a literary critic\footnote{Even at the Chicago Literary book club, he argues for a more sensuous approach to cognition: \emph{"In the world of physics, if we are to have any knowledge of that world, there must be nervous impulses in our heads which happen only if the worlds excites our eyes, ears, nose or skin."} \citep{mcculloch_delusion_1953}}.

Going further in the processes of the brain, he indeed finds out, in another paper with Letvinn and Pitts \citep{lettvin_what_1959}, that the organs through which the world excites the brain \emph{are themselves} agents of process, activating a series of probabilistic techniques, such as noise reduction and softmax, to provide a signal to the brain which isn't the untouched, unary, \emph{symbolical} version of the signal input by the external stimuli, and nor does it seem to turn it into such.

We see here the development of a theory for a situated, embodied stance towards cognition. In it, the senses are as essential as the brain for an understanding—that is, for the acquisition, through translation which then enable deliberate and successful action\footnote{A reversal one could observe around the same time at the moment of the publication of Wittgenstein's \emph{Philosophical Investigations}, where he disown his previous approach to language, and favors a more contextual, use-centered approach \citep{wittgenstein_recherches_2004}}.

\vspace*{1\baselineskip}

\begin{quote}
    Understanding internal definitions well enough to be sure a program means what we intend it to mean requires a more elaborate model of the evaluation process than we have presented in this chapter. \citep{abelson_structure_1979}
\end{quote}

A couple of decades later, Abelson and Sussman still note, in their introductory textbook to computer science, the difficulty to convey meaning mechanically. While formal notation is able to enable digital computation, it nonetheless proved to be limited when it came to accurately and expressively  conveying meaning. This limitation, of being able to express formally what we understand intuitively (e.g. \emph{what is a chair?}\footnote{A question addressed by Joseph Kosuth in his artwork \emph{One and Three Chairs}, 1965}) appeared as computers applications left the domain of logic and arithmetic.

After having seen the possibilities and limtations of making machines understand through the use of formal languages, and the shift towards sensory perception as a possible locus of cognitive processes, we now turn to these ways of knowing that exist in humans beyond language.

\subsection{Knowing-what and knowing-how} %10k
% then simon penny and tacit knowledge

% and lakoff

\begin{quote}
    Thinking is a continuous struggle between conceptual order and empirical comprehensiveness - wright mills, intellectual craftsmanship is an example of the process of understanding a topic in the context of reserch. it's not just purely formal, and is aided by technical appendixes
\end{quote}

\begin{itemize}
    \item jones and reckoning with matter would be a nice transition?
    \item cecil wright
    \item simon penny
    \item polanyi
    \item EXAMPLE: XML (applen, mcdaniel, rhetorics of xml)
\end{itemize}

% first the computer
\section{Understanding computation}

Why is it so, that two notions of understanding exist? What makes it challenging to understand computation? We'll look at it from a more abstract point of view, investigating the ontological status of software. This will highlight some of the theoretical properties that make it hard to understand, such as its relation to hardware, its relation to a specification, and its relation to time and space

We will also look at the practical investigations of how people understand software by looking into the psychology and cognition of programmers, with various studies and books on the subject.

% the software effect (combination of knowing what (it does) and how (it does it))
% the philosophical enquiries

\subsection{Software ontology}

\begin{itemize}
    \item lando, lapujade, towards a general ontology of computer programs
    \item irmak, software is an abstract artifact
    \item suber, what is software
    \item cantell smith age of significance introduction
    \item berry philosophy of software
    \item turner, computational artifacts
    \item rapaport
    \item simondon yallah
\end{itemize}

\subsection{The psychology of programming}
% the studies

\begin{itemize}
    \item dexter, dolese, seidel, kozbelt
    \item ivanova, comprehension of computer code
    \item detienne: software design, cognitive aspects
    \item weinberg
    \item petre
\end{itemize}

% then the human mind
\section{Means of understanding}

As we've seen in the previous sequence, there are specific problems to understanding computation, but there are also ways of achieving a good understanding of a program.

We'll look a bit deeper at some of those techniques, specifically metaphors and tools.

% then the communcation between both
\subsection{Metaphors of computation}

\begin{itemize}
    \item sally wyatt
    \item also some shit about interfaces (nielsen)
\end{itemize}

\subsection{Tools as an extension}
% IDEs

\begin{itemize}
    \item fishwick aesthetic progamming
    \item starting with the basics: goody, logic of writing
    \item allamanis, using ML for code generation and analysis, and mattt (as we may code) highlights the need for such a thing (What if, instead of lowering source code down for the purpose of execution, we raised source code for the purpose of understanding?)
    \item barker writing software documentation
    \item wilken card index
\end{itemize}

% The computer as prosthetic organ, distributed cognition

\pagebreak

Conclusion: we circle back to aesthetics, and we look at the things that are both tools and metaphors: programming languages. We will see what roles metaphors play; and, if linguistics is a key component in the writing of clear source code, we should also look at programming languages.